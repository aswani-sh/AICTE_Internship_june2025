{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJpejy93Nk376ASy2EEM6J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aswani-sh/AICTE_Internship_june2025/blob/main/Garbageclassificationweek2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2q-AFmCoWEK",
        "outputId": "dabf1042-c693-48d7-98ad-b6cd17e5d9ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Found 18 files belonging to 1 classes.\n",
            "Using 15 files for training.\n",
            "Found 18 files belonging to 1 classes.\n",
            "Using 3 files for validation.\n",
            "Classes: ['garbage']\n",
            "TrashType_Image_Dataset/\n",
            "  garbage/\n",
            "    TrashType_Image_Dataset/\n",
            "      cardboard/\n",
            "      glass/\n",
            "      plastic/\n",
            "        plastic_017.jpg\n",
            "        plastic_005.jpg\n",
            "        plastic_012.jpg\n",
            "        plastic_016.jpg\n",
            "        plastic_002.jpg\n",
            "      trash/\n",
            "      metal/\n",
            "      paper/\n",
            "âœ… Detected dataset directory: /content/garbage_dataset\n",
            "['garbage']\n",
            "['garbage']\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import Layer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Rescaling, GlobalAveragePooling2D\n",
        "from tensorflow.keras import layers, optimizers, callbacks\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from tensorflow.keras.applications import EfficientNetV2B2\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import gradio as gr\n",
        "import os\n",
        "\n",
        "\n",
        "# --- For Google Drive ---\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "dataset_dir = \"/content/garbage_dataset\"\n",
        "  # ðŸ” Replace this with your actual path\n",
        "\n",
        "import zipfile\n",
        "\n",
        "# Unzip the dataset\n",
        "zip_path = \"/content/drive/MyDrive/garbage (1).zip\"  #path\n",
        "extract_path =  \"/content/garbage_dataset/TrashType_Image_Dataset\"\n",
        "     # Folder where ZIP contents will be extracted\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "dataset_dir = extract_path  # Point to the extracted folder, not the zip file\n",
        "\n",
        "\n",
        "# Dataset parameters\n",
        "image_size = (124, 124)\n",
        "batch_size = 32\n",
        "seed = 42\n",
        "\n",
        "# Load datasets\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    dataset_dir,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=seed,\n",
        "    shuffle=True,\n",
        "    image_size=image_size,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    dataset_dir,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=seed,\n",
        "    shuffle=True,\n",
        "    image_size=image_size,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "val_class = val_ds.class_names\n",
        "print(\"Classes:\", val_class)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "for root, dirs, files in os.walk(dataset_dir):\n",
        "    level = root.replace(dataset_dir, '').count(os.sep)\n",
        "    indent = ' ' * 2 * level\n",
        "    print(f\"{indent}{os.path.basename(root)}/\")\n",
        "    subindent = ' ' * 2 * (level + 1)\n",
        "    for f in files[:5]:  # show first 5 files only\n",
        "        print(f\"{subindent}{f}\")\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "def find_dataset_root(base_path):\n",
        "    for root, dirs, files in os.walk(base_path):\n",
        "        # true root: no image files directly here but multiple subfolders\n",
        "        if len(dirs) >= 2 and all(os.path.isdir(os.path.join(root, d)) for d in dirs) and not any(f.lower().endswith(('.jpg','.png')) for f in files):\n",
        "            return root\n",
        "    return None\n",
        "\n",
        "dataset_dir = find_dataset_root(\"/content/garbage_dataset\")\n",
        "print(\"âœ… Detected dataset directory:\", dataset_dir)  # Should print path ending with .../TrashType_Image_Dataset\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Get the total number of batches in the validation dataset\n",
        "val_batches = tf.data.experimental.cardinality(val_ds)\n",
        "\n",
        "# Split the validation dataset into two equal parts:\n",
        "# First half becomes the test dataset\n",
        "test_ds = val_ds.take(val_batches // 2)\n",
        "\n",
        "# Second half remains as the validation dataset\n",
        "val_dat = val_ds.skip(val_batches // 2)\n",
        "\n",
        "# Optimize test dataset by caching and prefetching to improve performance\n",
        "test_ds_eval = test_ds.cache().prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "\n",
        "print(train_ds.class_names)\n",
        "print(val_class)\n",
        "print(len(train_ds.class_names))"
      ]
    }
  ]
}